{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Presented by: Reza Saadatyar <br/>\n",
    "E-mail: Reza.Saadatyar92@gmail.com"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Importing the required Libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 813,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "from scipy import signal\n",
    "import matplotlib.pyplot as plt\n",
    "from xgboost import XGBClassifier\n",
    "from matplotlib.colors import ListedColormap\n",
    "from sklearn import datasets, preprocessing, model_selection, linear_model, neural_network, svm, tree, naive_bayes, neighbors, ensemble, discriminant_analysis\n",
    "# from sklearn import model_selection,  multiclass\n",
    "# from Plot_decision_regions import plot_decision_regions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Functions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 829,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================== Preparing data =========================================================\n",
    "def preparing_data(data, labels):\n",
    "    if data.shape[0] < data.shape[1]:\n",
    "        data = data.T\n",
    "    Labels = preprocessing.LabelEncoder()\n",
    "    Labels = Labels.fit_transform(labels)\n",
    "    return data, Labels\n",
    " \n",
    "# ==================================================== Filtering =========================================================\n",
    "def filtering(data, f_low, f_high, order, fs, filter_type=\"low\"):\n",
    "    if data.ndim > 1:\n",
    "        if data.shape[0] > data.shape[1]:\n",
    "            data = data.T\n",
    "            \n",
    "    f_low = f_low / (fs / 2)\n",
    "    f_high = f_high / (fs / 2)\n",
    "    \n",
    "    if filter_type == \"low\":\n",
    "        b, a = signal.butter(order, f_low, btype='low')\n",
    "    elif filter_type == \"high\":\n",
    "        b, a = signal.butter(order, f_high, btype='high')\n",
    "    elif filter_type == \"bandpass\":\n",
    "        b, a = signal.butter(order, [f_low, f_high], btype='bandpass')\n",
    "    elif filter_type == \"bandstop\":\n",
    "        b, a = signal.butter(order, [f_low, f_high], btype='bandstop')\n",
    "    filtered_data = signal.filtfilt(b, a, data)\n",
    "    \n",
    "    return filtered_data.T\n",
    "\n",
    "# ================================================= Plot data ============================================================\n",
    "def plot_data(filtered_data, fs=None, first_point=0, last_point=100, val_ylim='', size_fig=(7,5), title='', display_figure=\"off\"):\n",
    "    \n",
    "    if display_figure == \"on\":\n",
    "        if filtered_data.shape[0] < filtered_data.shape[1]:\n",
    "            filtered_data = filtered_data.T\n",
    "            \n",
    "        filtered_data = filtered_data[first_point:last_point,:]\n",
    "        \n",
    "        std = np.sort(np.std(filtered_data, axis=0))\n",
    "        if len(std) > 100:\n",
    "            std = np.mean(std[1:len(std)-1])\n",
    "        else:\n",
    "            std = np.mean(std)\n",
    "        \n",
    "        _, axs = plt.subplots(nrows=1, sharey='row', figsize=size_fig)\n",
    "        offset = np.arange(filtered_data.shape[1]*std*val_ylim, 1, -std*val_ylim)\n",
    "      \n",
    "        if fs is not None and np.array(fs) > 0:\n",
    "            time = (np.linspace(start=first_point/fs, stop=last_point/fs, num=len(filtered_data))).flatten()\n",
    "            line = axs.plot(time, filtered_data + offset, linewidth=1)\n",
    "            axs.set_xlabel('Time (sec)', fontsize=10)\n",
    "        else:\n",
    "            line = axs.plot(filtered_data + offset, linewidth=1)\n",
    "            axs.set_xlabel('sample', fontsize=10)\n",
    "        \n",
    "        axs.set_title(title, fontsize=10)\n",
    "        axs.set_yticks(offset)\n",
    "        axs.set_yticklabels([\"ch\" + str(i) for i in range(1, filtered_data.shape[1] + 1)],  weight='bold')\n",
    "        axs.tick_params(axis='x', labelsize=9)\n",
    "        axs.tick_params(axis='y', labelsize=8)\n",
    "        axs.set_ylabel('Channels', fontsize=10)\n",
    "        axs.tick_params(axis='y', color='k', labelcolor='k')\n",
    "        axs.grid(False)\n",
    "        ytick_labels = plt.gca().get_yticklabels()\n",
    "        for i, label in enumerate(ytick_labels):\n",
    "            # line[i].set_color(line[i].get_color())\n",
    "            label.set_color(line[i].get_color())\n",
    "            \n",
    "        axs.autoscale(enable=True, axis=\"x\",tight=True)\n",
    "        min = np.min(np.min(filtered_data + offset, axis=0))\n",
    "        max = np.max(np.max(filtered_data + offset, axis=0))\n",
    "        axs.set_ylim(min + min*0.02, max + max*0.01)\n",
    "\n",
    "# ============================================== Data normalization ======================================================                          \n",
    "def data_normalization(x_train, x_test, method=1):\n",
    "    if x_train.ndim == 1:\n",
    "        x_train = x_train.reshape(-1, 1)\n",
    "        x_test = x_test.reshape(-1, 1)\n",
    "        \n",
    "    if method == 1:\n",
    "        norm = preprocessing.MinMaxScaler()\n",
    "    elif method == 2:\n",
    "        norm = preprocessing.StandardScaler()\n",
    "        \n",
    "    x_train = norm.fit_transform(x_train)\n",
    "    x_test = norm.transform(x_test)\n",
    "    \n",
    "    return x_train, x_test\n",
    "\n",
    "# ================================================ Plot features =========================================================   \n",
    "def plot_features(data, labels, fig_size=(4, 3), title=\"Data raw\"):\n",
    "   \n",
    "   lab = np.unique(labels)\n",
    "   colors = np.array(sns.color_palette(\"bright\", len(lab))) \n",
    " \n",
    "   fig = plt.figure(figsize=fig_size)\n",
    "\n",
    "   if data.shape[1] == 1:\n",
    "      \n",
    "      grid = plt.GridSpec(4, 4, hspace=0.06, wspace=0.06)\n",
    "      ax = fig.add_subplot(grid[1:, :3])\n",
    "      ax1 = fig.add_subplot(grid[0, :3], yticklabels=[], sharex=ax)\n",
    "      \n",
    "      for i in range(0, len(lab)):\n",
    "         \n",
    "         tim = np.linspace(np.min(data[labels == lab[i], 0]), np.max(data[labels == lab[i], 0]), num=len(data[labels == lab[i], 0]), retstep=True)\n",
    "         ax.plot(tim[0], data[labels == lab[i], 0], '.', markersize=10, color=colors[i, :])\n",
    "         \n",
    "         _, bins = np.histogram(data[labels == lab[i], 0], density=True)\n",
    "         ax1.plot(bins, stats.norm.pdf(bins, np.mean(data[labels == lab[i], 0]), np.std(data[labels == lab[i], 0])), linewidth=1.5, color=colors[i, :], label=lab[i])\n",
    "         ax1.fill_between(bins, y1=stats.norm.pdf(bins, np.mean(data[labels == lab[i], 0]), np.std(data[labels == lab[i], 0])), y2=0, alpha=0.4)\n",
    "         \n",
    "   elif data.shape[1] < 3:\n",
    "       \n",
    "       grid = plt.GridSpec(4, 4, hspace=0.06, wspace=0.06)\n",
    "       ax = fig.add_subplot(grid[1:, :3])\n",
    "       ax1 = fig.add_subplot(grid[0, :3], yticklabels=[], sharex=ax)\n",
    "       ax2 = fig.add_subplot(grid[1:, 3], xticklabels=[], sharey=ax)\n",
    "       \n",
    "       for i in range(0, len(lab)):\n",
    "           \n",
    "           ax.plot(data[labels == lab[i], 0], data[labels == lab[i], 1], '.', markersize=10, color=colors[i, :])\n",
    "           \n",
    "           _, bins = np.histogram(data[labels == lab[i], 0], density=True)\n",
    "           ax1.plot(bins, stats.norm.pdf(bins, np.mean(data[labels == lab[i], 0]), np.std(data[labels == lab[i], 0])), linewidth=1.5, color=colors[i, :], label=lab[i])\n",
    "           ax1.fill_between(bins, y1=stats.norm.pdf(bins, np.mean(data[labels == lab[i], 0]), np.std(data[labels == lab[i], 0])), y2=0, alpha=0.4)\n",
    "           \n",
    "           _, bins = np.histogram(data[labels == lab[i], 1], density=True)\n",
    "           ax2.plot(stats.norm.pdf(bins, np.mean(data[labels == lab[i], 1]), np.std(data[labels == lab[i], 1])), bins, linewidth=2.5, color=colors[i, :])\n",
    "           ax2.fill_betweenx(bins, stats.norm.pdf(bins, np.mean(data[labels == lab[i], 1]), np.std(data[labels == lab[i], 1])), 0, alpha=0.4, color=colors[i, :])\n",
    "           \n",
    "   elif data.shape[1] > 2:\n",
    "      \n",
    "    #   ax = fig.add_axes((0.02, -0.05, 0.9, 0.9), projection=\"3d\")\n",
    "    #   ax1 = fig.add_axes((0.22, 0.67, 0.52, 0.16))\n",
    "    #   ax2 = fig.add_axes((0.8, 0.18, 0.13, 0.47))\n",
    "    #   ax3 = fig.add_axes((-0.05, 0.18, 0.13, 0.47))\n",
    "      \n",
    "      grid = plt.GridSpec(4, 4, hspace=0.05, wspace=0.05)\n",
    "      ax = fig.add_subplot(grid[1:, 1:3], projection=\"3d\")\n",
    "      ax1 = fig.add_subplot(grid[0, 1:3], yticklabels=[], sharex=ax)\n",
    "      ax2 = fig.add_subplot(grid[1:, 3], xticklabels=[], sharey=ax)\n",
    "      ax3 = fig.add_subplot(grid[1:, 0:1], xticklabels=[], sharey=ax)\n",
    "      \n",
    "      for i in range(0, len(lab)):\n",
    "         \n",
    "         ax.plot3D(data[labels == lab[i], 0], data[labels == lab[i], 1], data[labels == lab[i], 2], '.', markersize=10, color=colors[i, :], label=lab[i])\n",
    "         \n",
    "         _, bins = np.histogram(data[labels == lab[i], 0], density=True)\n",
    "         ax1.plot(bins, stats.norm.pdf(bins, np.mean(data[labels == lab[i], 0]), np.std(data[labels == lab[i], 0])), linewidth=2.5, color=colors[i, :], label=lab[i])\n",
    "         ax1.fill_between(bins, y1=stats.norm.pdf(bins, np.mean(data[labels == lab[i], 0]), np.std(data[labels == lab[i], 0])), y2=0, alpha=0.4, color=colors[i, :])\n",
    "         \n",
    "         _, bins = np.histogram(data[labels == lab[i], 1], density=True)\n",
    "         ax2.plot(stats.norm.pdf(bins, np.mean(data[labels == lab[i], 1]), np.std(data[labels == lab[i], 1])), bins, linewidth=2.5, color=colors[i, :])\n",
    "         ax2.fill_betweenx(bins, stats.norm.pdf(bins, np.mean(data[labels == lab[i], 1]), np.std(data[labels == lab[i], 1])), 0, alpha=0.4, color=colors[i, :])\n",
    "         \n",
    "         _, bins = np.histogram(data[labels == lab[i], 2], density=True)\n",
    "         ax3.plot(-stats.norm.pdf(bins, np.mean(data[labels == lab[i], 2]), np.std(data[labels == lab[i], 2])), bins, linewidth=2.5, color=colors[i, :])\n",
    "         ax3.fill_betweenx(bins, 0, -stats.norm.pdf(bins, np.mean(data[labels == lab[i], 2]), np.std(data[labels == lab[i], 2])), alpha=0.4, color=colors[i, :])\n",
    "      \n",
    "      ax.view_init(5, -120)\n",
    "      \n",
    "      ax2.tick_params(bottom=False, top=False, labelbottom=False, right=False, left=False, labelleft=False)\n",
    "      ax3.tick_params(bottom=False, top=False, labelbottom=False, right=False, left=False, labelleft=False)\n",
    "      \n",
    "      ax.tick_params(axis='z', which='both', bottom=False, top=False, labelbottom=True, labeltop=False, pad=-2)\n",
    "      \n",
    "      ax.set_ylabel('Feature 2', fontsize=10,labelpad=0, rotation=90, va='center')\n",
    "      ax.set_zlabel('Feature 3', labelpad=-6, fontsize=10, va='center')\n",
    "      \n",
    "      ax2.spines[['top', 'right', 'bottom']].set_visible(False)\n",
    "      ax3.spines[['top', 'bottom', 'left']].set_visible(False)\n",
    "\n",
    "#    ax.grid(visible=\"on\")\n",
    "\n",
    "   ax.set_xlabel('Feature 1', labelpad=8, fontsize=10, va='center'), ax.set_title(title, fontsize=10, pad=0, y=1)\n",
    "   ax.tick_params(axis='x', length=1.5, width=1, which='both', bottom=True, top=False, labelbottom=True, labeltop=False, pad=0.5)\n",
    "   ax.tick_params(axis='y', length=1.5, width=1, which=\"both\", bottom=False, top=False, labelbottom=True, labeltop=True, pad=0.5)\n",
    "    \n",
    "   ax1.spines[['top', 'left', 'right']].set_visible(False),    \n",
    "   ax1.tick_params(bottom=False, top=False, labelbottom=False, right=False, left=False, labelleft=False)\n",
    "   ax1.legend(title='Class', loc=\"best\", ncol=3, handlelength=0.3, handletextpad=0.2, fontsize=9)  # bbox_to_anchor=(0.1, pos1.x1-0.02, pos1.x1-0.02, 0)\n",
    "   \n",
    "   if data.shape[1] > 1:\n",
    "       ax.set_ylabel('Feature 2', labelpad=-1, fontsize=10, va='center')\n",
    "       \n",
    "       ax2.spines[['top', 'right', 'bottom']].set_visible(False)\n",
    "       ax2.tick_params(bottom=False, top=False, labelbottom=False, right=False, left=False, labelleft=False)\n",
    "    # ax.dist = 9\n",
    "    \n",
    "#     # plt.subplots_adjust(top=1, bottom=0, left=0, right=1, wspace=0.05, hspace=0), plt.autoscale(enable=True, axis=\"x\", tight=True), \n",
    "   \n",
    "\n",
    "#    ax.tick_params(axis='x', which='both', bottom=False, top=False, labelbottom=True, labeltop=False, pad=-2)\n",
    "#    ax.tick_params(axis='y', width=0, which=\"major\", bottom=False, top=False, labelbottom=True, labeltop=True, pad=-4, rotation=45)\n",
    "   \n",
    "#   ax.margins(x=0), ax.margins(y=0), ax.margins(z=0)\n",
    "\n",
    "# plt.autoscale(enable=True, axis=\"x\",tight=True)\n",
    "# fig.subplots_adjust(top=1, bottom=0, left=0, right=1, wspace=0, hspace=0)\n",
    "# \n",
    "#    ax.set_xlabel('Feature 1', labelpad=3, fontsize=10, va='center')\n",
    "#    ax.tick_params(axis='x', which='both', bottom=False, top=False, labelbottom=True, labeltop=False, pad=-2)\n",
    "#    ax.tick_params(axis='y', width=0, which=\"major\", bottom=False, top=False, labelbottom=True, labeltop=True, pad=-4, rotation=45)\n",
    "\n",
    "#    plt.autoscale(enable=True, axis=\"x\",tight=True)\n",
    "#    fig.subplots_adjust(top=1, bottom=0, left=0, right=1, wspace=0, hspace=0)\n",
    "#    pos1 = ax1.get_position()\n",
    "#    fig.suptitle(title, fontsize=10, y=pos1.y0 + 0.2)\n",
    "#    ax.yaxis.set_ticks(np.linspace(ax.get_yticks()[1], ax.get_yticks()[-2], int(len(ax.get_yticks()) / 2), dtype='int'))\n",
    "# ax.tick_params(direction='in', length=6, width=2, colors='grey', grid_color='r', grid_alpha=0.5)\n",
    "\n",
    "# ================================================ Plot_classification ===================================================  \n",
    "def plot_classification(data_train, label_train, data_test, label_test, model, type_class, position_title=0.05, fig_size=(3,3)):\n",
    "\n",
    "    resolution = 0.03\n",
    "    lab = np.unique(label_train)\n",
    "\n",
    "    x_combined = np.vstack((data_train, data_test))\n",
    "    y_combined = np.hstack((label_train, label_test))\n",
    "    x1_min, x1_max = x_combined[:, 0].min() - 1, x_combined[:, 0].max() + 1\n",
    "    x2_min, x2_max = x_combined[:, 1].min() - 1, x_combined[:, 1].max() + 1\n",
    "    xx1, xx2 = np.meshgrid(np.arange(x1_min, x1_max, resolution), np.arange(x2_min, x2_max, resolution))\n",
    "    z = model.predict(np.array([xx1.ravel(), xx2.ravel()]).T)\n",
    "    # --------------------------------------------------- Result Plot ---------------------------------------------------\n",
    "    fig, axs = plt.subplots(1, 2, sharey='row', figsize=fig_size)\n",
    "   \n",
    "    colors = list(reversed(sns.color_palette(\"bright\", len(lab)).as_hex()))\n",
    "    cmp = ListedColormap(colors[:len(lab)])\n",
    "    \n",
    "    axs[0].contourf(xx1, xx2, z.reshape(xx1.shape), alpha=0.35, cmap=cmp)\n",
    "    axs[0].set_xlim(xx1.min(), xx1.max())\n",
    "    axs[0].set_ylim(xx2.min(), xx2.max())\n",
    "\n",
    "    axs[1].contourf(xx1, xx2, z.reshape(xx1.shape), alpha=0.35, cmap=cmp)\n",
    "    axs[1].set_xlim(xx1.min(), xx1.max())\n",
    "    axs[1].set_ylim(xx2.min(), xx2.max())\n",
    "\n",
    "    for i in range(0, len(np.unique(y_combined))):\n",
    "        axs[0].plot(data_train[label_train == lab[i], 0], data_train[label_train == lab[i], 1], '.', color=colors[i], label=lab[i], markersize=10)\n",
    "        axs[1].plot(data_test[label_test == lab[i], 0], data_test[label_test == lab[i], 1], '.', color=colors[i], label=lab[i], markersize=10)\n",
    "    \n",
    "    axs[0].tick_params(axis='x', length=1.5, width=1, which='both', bottom=True, top=False, labelbottom=True, labeltop=False, pad=0.5)\n",
    "    axs[0].tick_params(axis='y', length=1.5, width=1, which=\"both\", bottom=False, top=False, labelbottom=True, labeltop=True, pad=0.5)\n",
    "    axs[1].tick_params(axis='x', length=1.5, width=1, which='both', bottom=True, top=False, labelbottom=True, labeltop=False, pad=0.5)\n",
    "    axs[1].tick_params(axis='y', length=1.5, width=1, which=\"both\", bottom=False, top=False, labelbottom=True, labeltop=True, pad=0.5)\n",
    "    \n",
    "    axs[0].set_xlabel('Feature 1',  fontsize=10, va='center'), axs[0].set_ylabel('Feature 2', labelpad=-1, fontsize=10, va='center')\n",
    "    axs[0].set_title('Training', loc='left', pad=0, fontsize=10), axs[1].set_xlabel('Feature 1',  fontsize=10, va='center')\n",
    "    axs[1].set_title('Test', loc='right', pad=0, fontsize=10), axs[0].legend(title='Class', ncol=3, handlelength=-0.1, handletextpad=0.3)\n",
    "    \n",
    "    plt.subplots_adjust(top=1, bottom=0, left=0, right=1, wspace=0.05, hspace=0), plt.autoscale(enable=True, axis=\"x\", tight=True), \n",
    "    fig.suptitle(type_class, fontsize=11,x=0.51,  y=1+position_title, fontweight='normal', color='black', va='top')\n",
    "    \n",
    "    # ax.tick_params(direction='in', length=6, width=2, colors='grey', grid_color='r', grid_alpha=0.5)\n",
    "    \n",
    "# =================================================== KNN_optimal ========================================================  \n",
    "def knn_optimal(data_train, label_train, data_test, label_test, n=21, fig_size=(3,2)):\n",
    "    if np.shape(data_train)[0] < np.shape(data_train)[1]:  # Convert Data training & Test >>>> m*n; m > n\n",
    "        data_train = data_train.T\n",
    "        data_test = data_test.T\n",
    "    t = np.arange(1, n)\n",
    "    accuracy_train = np.zeros(n-1)\n",
    "    accuracy_test = np.zeros(n-1)\n",
    "    for i in range(1 , n):\n",
    "        model = neighbors.KNeighborsClassifier(metric='minkowski', n_neighbors=i)\n",
    "        model.fit(data_train, label_train)\n",
    "        accuracy_train[i-1] = model.score(data_train, label_train)\n",
    "        accuracy_test[i-1] = model.score(data_test, label_test)\n",
    "    \n",
    "    plt.figure(figsize=fig_size)\n",
    "    plt.plot(t, accuracy_train, label=\"Training\")\n",
    "    plt.plot(t, accuracy_test, label=\"Test\")\n",
    "    plt.xticks(t)\n",
    "    plt.legend(fontsize=8)\n",
    "    \n",
    "    plt.xlabel(\"Number of neighbors\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.title(\"KNN\", fontsize=10)\n",
    "    plt.tick_params(axis='x', rotation=90)\n",
    "\n",
    "    return t[np.argmax(accuracy_test)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax2.spines[['top', 'right', 'bottom']].set_visible(False)\n",
    "ax2.tick_params(bottom=False, top=False, labelbottom=False, right=False, left=False, labelleft=False)\n",
    "\n",
    "ax.grid(visible=\"on\")\n",
    "ax.set_xlabel('Feature 1', labelpad=3, fontsize=10, va='center')\n",
    "ax.tick_params(axis='x', which='both', bottom=False, top=False, labelbottom=True, labeltop=False, pad=-2)\n",
    "ax.tick_params(axis='y', width=0, which=\"major\", bottom=False, top=False, labelbottom=True, labeltop=True, pad=-4, rotation=45)\n",
    "\n",
    "ax1.spines[['top', 'left', 'right']].set_visible(False)   \n",
    "ax1.tick_params(bottom=False, top=False, labelbottom=False, right=False, left=False, labelleft=False)\n",
    "ax1.legend(title='Class', loc=\"best\", ncol=3, handlelength=0.3, handletextpad=0.2, fontsize=9)  # bbox_to_anchor=(0.1, pos1.x1-0.02, pos1.x1-0.02, 0)\n",
    "\n",
    "plt.autoscale(enable=True, axis=\"x\",tight=True)\n",
    "fig.subplots_adjust(top=1, bottom=0, left=0, right=1, wspace=0, hspace=0)\n",
    "ax1.set_title(\"title\", fontsize=10, pad=0, y=1)\n",
    "# pos1 = ax1.get_position()\n",
    "# fig.suptitle(\"title\", fontsize=10, x=0.4, y=pos1.y1)\n",
    "# print(pos1)\n",
    "       \n",
    "  \n",
    "      ax2.spines[['top', 'right', 'bottom']].set_visible(False)\n",
    "      ax2.tick_params(bottom=False, top=False, labelbottom=False, right=False, left=False, labelleft=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. Load data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 643,
   "metadata": {},
   "outputs": [],
   "source": [
    "method = 1\n",
    "if method==1:\n",
    "   x, y = datasets.make_classification(n_samples=1000,      # Number of samples in the dataset\n",
    "                                       n_features=5,       # Number of total features\n",
    "                                       n_informative=4,    # Number of informative features\n",
    "                                       n_redundant=0,      # Number of redundant features\n",
    "                                       n_classes=3,        # Number of classes in the dataset (binary classification in this case)\n",
    "                                       random_state=1)     # Seed for reproducibility)\n",
    "   # print(f\"{x.shape=},\\n{x=},\\n{y=}\")\n",
    "elif method==2:\n",
    "   iris = datasets.load_iris()\n",
    "   x = iris.data\n",
    "   y = iris.target\n",
    "   # print(f\"{iris.data.shape=},\\n{iris.feature_names=},\\n{iris.target_names=},\\n{iris.data=},\\n{iris.target}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. Preparing the data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data, labels = preparing_data(x, y)\n",
    "data=data[:,0:3]\n",
    "\n",
    "plot_features(data, labels, fig_size=(4, 3), title=\"Data raw\")\n",
    "# ------------------------------------------------------ Boxplot -------------------------------------------------------------\n",
    "plt.figure(figsize=(5, 3))\n",
    "plt.boxplot(data, boxprops=dict(color='green'), whiskerprops=dict(color='red'), medianprops=dict(color='blue'), capprops=dict(color='gray'), patch_artist=True)\n",
    "plt.xlabel(\"Channels\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = [0, 0]\n",
    "cov = [[1, 1], [1, 2]]\n",
    "x, y = np.random.multivariate_normal(mean, cov, 3000).T\n",
    "\n",
    "# Set up the axes with gridspec\n",
    "fig = plt.figure(figsize=(5, 3))\n",
    "grid = plt.GridSpec(4, 4, hspace=0.05, wspace=0.05)\n",
    "\n",
    "ax = fig.add_subplot(grid[1:, :3])\n",
    "ax1 = fig.add_subplot(grid[0, :3], yticklabels=[], sharex=ax)\n",
    "ax2 = fig.add_subplot(grid[1:, 3], xticklabels=[], sharey=ax)\n",
    "ax3 = fig.add_subplot(grid[1:, 3], xticklabels=[], sharey=ax)\n",
    "\n",
    "lab = np.unique(labels)\n",
    "colors = np.array(sns.color_palette(\"bright\", len(lab))) \n",
    "    \n",
    "# for i in range(0, len(lab)):\n",
    "   \n",
    "#    ax.plot(data[labels == lab[i], 0], data[labels == lab[i], 1], '.', markersize=10, color=colors[i, :])\n",
    "   \n",
    "#    _, bins = np.histogram(data[labels == lab[i], 0], density=True)\n",
    "#    ax1.plot(bins, stats.norm.pdf(bins, np.mean(data[labels == lab[i], 0]), np.std(data[labels == lab[i], 0])), linewidth=1.5, color=colors[i, :], label=lab[i])\n",
    "#    ax1.fill_between(bins, y1=stats.norm.pdf(bins, np.mean(data[labels == lab[i], 0]), np.std(data[labels == lab[i], 0])), y2=0, alpha=0.4)\n",
    "   \n",
    "#    _, bins = np.histogram(data[labels == lab[i], 1], density=True)\n",
    "#    ax2.plot(stats.norm.pdf(bins, np.mean(data[labels == lab[i], 1]), np.std(data[labels == lab[i], 1])), bins, linewidth=2.5, color=colors[i, :])\n",
    "#    ax2.fill_betweenx(bins, stats.norm.pdf(bins, np.mean(data[labels == lab[i], 1]), np.std(data[labels == lab[i], 1])), 0, alpha=0.4, color=colors[i, :])\n",
    "\n",
    "# ax.set_ylabel('Feature 2', fontsize=10,labelpad=0, va='center')\n",
    "# ax2.spines[['top', 'right', 'bottom']].set_visible(False)\n",
    "# ax2.tick_params(bottom=False, top=False, labelbottom=False, right=False, left=False, labelleft=False)\n",
    "\n",
    "# ax.grid(visible=\"on\")\n",
    "# ax.set_xlabel('Feature 1', labelpad=3, fontsize=10, va='center')\n",
    "# ax.tick_params(axis='x', which='both', bottom=False, top=False, labelbottom=True, labeltop=False, pad=-2)\n",
    "# ax.tick_params(axis='y', width=0, which=\"major\", bottom=False, top=False, labelbottom=True, labeltop=True, pad=-4, rotation=45)\n",
    "\n",
    "# ax1.spines[['top', 'left', 'right']].set_visible(False)   \n",
    "# ax1.tick_params(bottom=False, top=False, labelbottom=False, right=False, left=False, labelleft=False)\n",
    "# ax1.legend(title='Class', loc=\"best\", ncol=3, handlelength=0.3, handletextpad=0.2, fontsize=9)  # bbox_to_anchor=(0.1, pos1.x1-0.02, pos1.x1-0.02, 0)\n",
    "\n",
    "# plt.autoscale(enable=True, axis=\"x\",tight=True)\n",
    "# fig.subplots_adjust(top=1, bottom=0, left=0, right=1, wspace=0, hspace=0)\n",
    "# ax1.set_title(\"title\", fontsize=10, pad=0, y=1)\n",
    "# # pos1 = ax1.get_position()\n",
    "# # fig.suptitle(\"title\", fontsize=10, x=0.4, y=pos1.y1)\n",
    "# # print(pos1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. Filtering**<br/>\n",
    "\n",
    "`Fs > f_high > f_low`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_data = filtering(data, f_low=3, f_high=10, order=3, fs=50, filter_type=\"low\")    # btype:'low','high','bandpass','bandstop'\n",
    "plot_data(filtered_data, fs=None, first_point=0, last_point=500, val_ylim=3, size_fig=(7,5), title='filtered_data', display_figure=\"off\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4. Split data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=data[:,0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train, data_test, label_train, label_test = model_selection.train_test_split(data, labels, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5. Data normalization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train, data_test = data_normalization(data_train, data_test, method=2)   # method 1: MinMaxScaler, method 2: StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**6. Classification**<br/>\n",
    "\n",
    "The advantages of support vector machines are:\n",
    "- Effective in high dimensional spaces.\n",
    "- Still effective in cases where number of dimensions is greater than the number of samples.\n",
    "- Uses a subset of training points in the decision function (called support vectors), so it is also memory efficient.\n",
    "- Versatile: different Kernel functions can be specified for the decision function. Common kernels are provided, but it is also possible to specify custom kernels.\n",
    "\n",
    "Decision Tree:\n",
    "- def error(p):<br/>\n",
    "    return 1 - np.max([p, 1 - p])\n",
    "- def gini(p):<br/>\n",
    "    return (p)*(1 - (p)) + (1 - p)*(1 - (1-p))\n",
    "- def entropy(p):<br/>\n",
    "    return - p*np.log2(p) - (1 - p)*np.log2((1 - p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 830,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR_train: 0.5257142857142857, LR_test: 0.5366666666666666\n",
      "MLP_train: 0.5414285714285715, MLP_test: 0.5566666666666666\n",
      "SVM_train: 0.5171428571428571, SVM_test: 0.5066666666666667\n",
      "DT_train: 0.7257142857142858, DT_test: 0.6233333333333333\n",
      "NB_train: 0.5357142857142857, Nb_test: 0.58\n",
      "RF_train: 0.77, RF_test: 0.7166666666666667\n",
      "AdaBoost_train: 0.6357142857142857, AdaBoost_test: 0.49666666666666665\n",
      "XGBoost_train: 1.0, XGBoost_test: 0.7066666666666667\n",
      "LDA_train: 0.5314285714285715, LDA_test: 0.54\n",
      "KNN_train: 0.7357142857142858, KNN_test: 0.7633333333333333\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 400x250 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ------------------------------------------------ Logistic Regression ----------------------------------------------------\n",
    "model = linear_model.LogisticRegression(C=1e5, solver='lbfgs', multi_class='multinomial', penalty='l2') # sag, newton-c,lbfg;  penalty='l2', Nonr\n",
    "model.fit(data_train, label_train)\n",
    "\n",
    "y_pred_train = model.predict(data_train) \n",
    "y_pred_test = model.predict(data_test)    #predict model\n",
    "print(f\"LR_train: {model.score(data_train, label_train)}, LR_test: {model.score(data_test, label_test)}\")\n",
    "\n",
    "# ------------------------------------------------------- MLP -------------------------------------------------------------\n",
    "model = neural_network.MLPClassifier(hidden_layer_sizes=(8), max_iter=1000, alpha=1e-4, learning_rate='invscaling', solver='sgd',\n",
    "                                     random_state=1, learning_rate_init=0.05, verbose=False , tol=1e-4)    # {'adam', 'sgd', 'lbfgs'}\n",
    "model.fit(data_train, label_train)\n",
    "y_pred_train = model.predict(data_train) \n",
    "y_pred_test = model.predict(data_test)    #predict model\n",
    "print(f\"MLP_train: {model.score(data_train, label_train)}, MLP_test: {model.score(data_test, label_test)}\")\n",
    "\n",
    "# ------------------------------------------------------- SVM -------------------------------------------------------------\n",
    "kernel_svm = \"linear\"   # poly, rbf, sigmoid, precomputed, if 'poly'---> degree=3\n",
    "# c ---> Regularization parameter\n",
    "\n",
    "model = svm.SVC(kernel=kernel_svm, random_state=0, C=10, gamma=\"auto\", probability=True) \n",
    "model.fit(data_train, label_train)\n",
    "model.support_vectors_\n",
    "model.support_      # get indices of support vectors\n",
    "model.n_support_  # get number of support vectors for each class\n",
    "y_pred_train = model.predict(data_train) \n",
    "y_pred_test = model.predict(data_test)    #predict model\n",
    "print(f\"SVM_train: {model.score(data_train, label_train)}, SVM_test: {model.score(data_test, label_test)}\")\n",
    "\n",
    "# -------------------------------------------------------- DT -------------------------------------------------------------\n",
    "criterion_dt = \"gini\"                   #  gini, entropy, log_loss\n",
    "model = tree.DecisionTreeClassifier(criterion=criterion_dt, max_depth=5, random_state=0)\n",
    "model.fit(data_train, label_train)\n",
    "y_pred_train = model.predict(data_train) \n",
    "y_pred_test = model.predict(data_test)    #predict model\n",
    "print(f\"DT_train: {model.score(data_train, label_train)}, DT_test: {model.score(data_test, label_test)}\")\n",
    "\n",
    "# -------------------------------------------------------- NB -------------------------------------------------------------\n",
    "model = naive_bayes.GaussianNB()\n",
    "model.fit(data_train, label_train)\n",
    "y_pred_train = model.predict(data_train) \n",
    "y_pred_test = model.predict(data_test)    #predict model\n",
    "print(f\"NB_train: {model.score(data_train, label_train)}, Nb_test: {model.score(data_test, label_test)}\") \n",
    "\n",
    "# -------------------------------------------------------- RF -------------------------------------------------------------       \n",
    "model = ensemble.RandomForestClassifier(n_estimators=500, max_depth=5, criterion=criterion_dt, random_state=0)\n",
    "model.fit(data_train, label_train)\n",
    "y_pred_train = model.predict(data_train) \n",
    "y_pred_test = model.predict(data_test)    #predict model\n",
    "print(f\"RF_train: {model.score(data_train, label_train)}, RF_test: {model.score(data_test, label_test)}\") \n",
    "\n",
    "# ---------------------------------------------------- AdaBoost -----------------------------------------------------------  \n",
    " # model = tree.DecisionTreeClassifier(criterion='entropy', max_depth=3, random_state=0)\n",
    "# model = ensemble.AdaBoostClassifier(base_estimator=model, n_estimators=100, random_state=0)\n",
    "model = ensemble.AdaBoostClassifier(n_estimators=500, learning_rate=0.9, random_state=0)\n",
    "model.fit(data_train, label_train)\n",
    "y_pred_train = model.predict(data_train) \n",
    "y_pred_test = model.predict(data_test)    #predict model\n",
    "print(f\"AdaBoost_train: {model.score(data_train, label_train)}, AdaBoost_test: {model.score(data_test, label_test)}\") \n",
    "\n",
    "# ------------------------------------------------------ XGBoost ----------------------------------------------------------  \n",
    "model = XGBClassifier(max_depth=5, n_estimators=500, learning_rate=0.08, random_state=0, objective='multi:softprob')\n",
    "model.fit(data_train, label_train)\n",
    "y_pred_train = model.predict(data_train) \n",
    "y_pred_test = model.predict(data_test)    #predict model\n",
    "print(f\"XGBoost_train: {model.score(data_train, label_train)}, XGBoost_test: {model.score(data_test, label_test)}\") \n",
    "\n",
    "# -------------------------------------------------------- LDA ------------------------------------------------------------  \n",
    "model = discriminant_analysis.LinearDiscriminantAnalysis(n_components=len(np.unique(labels)) - 1)\n",
    "model.fit(data_train, label_train)\n",
    "y_pred_train = model.predict(data_train) \n",
    "y_pred_test = model.predict(data_test)    #predict model\n",
    "print(f\"LDA_train: {model.score(data_train, label_train)}, LDA_test: {model.score(data_test, label_test)}\") \n",
    "\n",
    "# -------------------------------------------------------- LDA ------------------------------------------------------------  \n",
    "num_k = knn_optimal(data_train, label_train, data_test, label_test, n=21, fig_size=(4,2.5))  # Obtain optimal K\n",
    "model = neighbors.KNeighborsClassifier(n_neighbors=num_k, metric='minkowski')\n",
    "model.fit(data_train, label_train)\n",
    "y_pred_train = model.predict(data_train) \n",
    "y_pred_test = model.predict(data_test)    #predict model\n",
    "print(f\"KNN_train: {model.score(data_train, label_train)}, KNN_test: {model.score(data_test, label_test)}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "naive_bayes, ensemble,  XGBClassifier, discriminant_analysis, neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type_class = \"Logistic Regression\"\n",
    "k_fold = 5\n",
    "\n",
    "if data_train.shape[1] == 2:\n",
    "        plot_classification(data_train, label_train, data_test, label_test, model, type_class, position_title=0.08, fig_size=(3, 2))\n",
    "        \n",
    "elif np.shape(data_train)[1] > 2:\n",
    "        x_train = data_train[:, 0:3]\n",
    "        x_test = data_test[:, 0:3]\n",
    "        lab = np.unique(label_train)\n",
    "        fig = plt.figure(figsize=(5, 3))\n",
    "        # fig, axs= plt.subplots(nrows=1, ncols=2,)\n",
    "        ax1 = fig.add_subplot(121, projection='3d')\n",
    "        ax2 = fig.add_subplot(122, projection='3d')\n",
    "\n",
    "        for i in range(0, len(lab)):\n",
    "            ax1.plot3D(x_train[y_pred_train == lab[i], 0], x_train[y_pred_train == lab[i], 1], x_train[y_pred_train == lab[i], 2], '.')\n",
    "            ax2.plot3D(x_test[y_pred_test == lab[i], 0], x_test[y_pred_test == lab[i], 1], x_test[y_pred_test == lab[i], 2], '.')\n",
    "        \n",
    "        ax1.view_init(5, -120), ax2.view_init(5, -120)    \n",
    "        # ax1.title.set_text('Training'), ax1.set_xlabel('Feature 1'), ax1.set_ylabel('Feature 2'), ax1.set_zlabel('Feature 3'),\n",
    "        # ax1.legend(lab, title='Class', ncol=3, handlelength=0.8, handletextpad=0.2), ax2.legend(lab, title='Class', ncol=3, handlelength=0.8, handletextpad=0.2)\n",
    "        # , ax2.title.set_text('Test'), ax2.set_xlabel('Feature 1'), ax2.set_ylabel('Feature 2'), ax2.set_zlabel('Feature 3'),\n",
    "        # pos1 = ax2.get_position()\n",
    "        # fig.suptitle('Classification Type: ' + type_class+'; '+str(k_fold)+'th K-fold:', fontsize=12, y=pos1.x1), fig.show()\n",
    "        \n",
    "        ax1.set_xlabel('Feature 1', labelpad=-1, fontsize=10, va='center'), ax2.set_xlabel('Feature 1', labelpad=-1, fontsize=10, va='center')\n",
    "        ax1.set_ylabel('Feature 2', labelpad=-1, fontsize=10, va='center'), ax2.set_ylabel('Feature 2', labelpad=-1, fontsize=10, va='center')\n",
    "        ax1.set_zlabel('Feature 3', labelpad=-6, fontsize=10, va='center'), ax2.set_zlabel('Feature 3', labelpad=-6, fontsize=10, va='center')\n",
    "        \n",
    "        ax1.tick_params(axis='x', length=1.5, width=1, which='both', bottom=True, top=False, labelbottom=True, labeltop=False, pad=-4)\n",
    "        ax2.tick_params(axis='x', length=1.5, width=1, which='both', bottom=True, top=False, labelbottom=True, labeltop=False, pad=-4)\n",
    "        ax1.tick_params(axis='y', length=2, width=1, which=\"both\", bottom=True, top=False, labelbottom=True, labeltop=True, pad=-4, rotation=45)\n",
    "        ax2.tick_params(axis='y', length=2, width=1, which=\"both\", bottom=True, top=False, labelbottom=True, labeltop=True, pad=-4, rotation=45)\n",
    "        ax1.tick_params(axis='z', length=2, width=1, which='both', bottom=False, top=False, labelbottom=True, labeltop=False, pad=-2)\n",
    "        ax2.tick_params(axis='z', length=2, width=1, which='both', bottom=False, top=False, labelbottom=True, labeltop=False, pad=-2)\n",
    "        \n",
    "        \n",
    "#       ax.margins(x=0), ax.margins(y=0), ax.margins(z=0)\n",
    "\n",
    "#    pos1 = ax1.get_position()\n",
    "#    fig.suptitle(title, fontsize=10, y=pos1.y0 + 0.2)\n",
    "\n",
    "#    ax1.legend(title='Class', loc=\"best\", ncol=3, handlelength=0.09, handletextpad=0.3, fontsize=8)  # bbox_to_anchor=(0.1, pos1.x1-0.02, pos1.x1-0.02, 0)\n",
    "#    fig.subplots_adjust(top=1, bottom=0, left=0, right=1, wspace=0, hspace=0)\n",
    "        \n",
    "#     axs[0].set_title('Training', loc='left', pad=0, fontsize=10), axs[1].set_xlabel('Feature 1',  fontsize=10, va='center')\n",
    "#     axs[1].set_title('Test', loc='right', pad=0, fontsize=10), axs[0].legend(title='Class', ncol=3, handlelength=-0.1, handletextpad=0.3)\n",
    "    \n",
    "#     fig.suptitle(type_class, fontsize=11,x=0.51,  y=1+position_title, fontweight='normal', color='black', va='top')\n",
    "        plt.subplots_adjust(top=0, bottom=-1, left=-1, right=0, wspace=0.02, hspace=0), plt.autoscale(enable=True, axis=\"x\", tight=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def plot_classifier(data_train, label_train, data_test, label_test, label_predict_train, label_predict_test, k_fold,  model, type_class):\n",
    "    if np.shape(data_train)[1] == 2:\n",
    "        plot_decision_regions(data_train, label_train, data_test, label_test, model, type_class)\n",
    "    elif np.shape(data_train)[1] == 3:\n",
    "        lab_tr = np.unique(label_train)\n",
    "        fig = plt.figure(figsize=(8, 5))\n",
    "        ax1 = fig.add_subplot(121, projection='3d')\n",
    "        ax2 = fig.add_subplot(122, projection='3d')\n",
    "\n",
    "        for i in range(0, len(lab_tr)):\n",
    "            ax1.plot3D(data_train[label_predict_train == lab_tr[i], 0], data_train[label_predict_train == lab_tr[i], 1], data_train[label_predict_train == lab_tr[i], 2], '.')\n",
    "            ax2.plot3D(data_test[label_predict_test == lab_tr[i], 0], data_test[label_predict_test == lab_tr[i], 1], data_test[label_predict_test == lab_tr[i], 2], '.')\n",
    "        ax1.title.set_text('Training'), ax1.set_xlabel('Feature 1'), ax1.set_ylabel('Feature 2'), ax1.set_zlabel('Feature 3'),\n",
    "        ax1.legend(lab_tr, title='Class', ncol=3, handlelength=0.8, handletextpad=0.2), ax2.legend(lab_tr, title='Class', ncol=3, handlelength=0.8, handletextpad=0.2)\n",
    "        ax1.view_init(30, 60), ax2.view_init(30, 60), ax2.title.set_text('Test'), ax2.set_xlabel('Feature 1'), ax2.set_ylabel('Feature 2'), ax2.set_zlabel('Feature 3'),\n",
    "        pos1 = ax2.get_position()\n",
    "        fig.suptitle('Classification Type: ' + type_class+'; '+str(k_fold)+'th K-fold:', fontsize=12, y=pos1.x1), fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn import datasets\n",
    "from Filtering import filtering\n",
    "from Clustering import clustering\n",
    "from Preparing_data import preparing_data\n",
    "from Normalize import normalize_data\n",
    "from Classification import classification\n",
    "from Plot_clustering import plot_cluster\n",
    "from Feature_Selection import featureselection\n",
    "from Feature_Extraction import feature_extraction\n",
    "from Output_Training_Test_Network import output_network\n",
    "\n",
    "\n",
    "# ======================================= Step 3: Feature Extraction & Selection =======================================\n",
    "# Data = feature_extraction(Data, Labels, number_feature=3, number_neighbors=70, type_feature='LDA')\n",
    "# Data = featureselection(Data, Labels, threshold=0.1, number_feature=3, c_l1fs=0.01, n_estimators_tfs=100,  type_feature='MI')\n",
    "\"\"\"\n",
    "Feature Extraction:\n",
    "PCA:Principal Component Analysis; LDA:Linear discriminant analysis; ICA: Independent component analysis; SVD: Singular value decomposition\n",
    "TSNE:T-distributed stochastic neighbor embedding; FA: Factor analysis; Isomap: Isometric Feature Mapping\n",
    "Feature Selection:\n",
    "Variance; Mutual information (MI); Chi-square test (Chi-square); fisher_score (FS); Forward feature selection (FFS);\n",
    "Backward feature selection (BFS); Exhaustive Feature Selection (EFS); Recursive feature elimination (RFE); Random Forest (RF)\n",
    "Univariate feature selection (UFS); L1-based feature selection (L1FS), Tree-based feature selection (TFS)\n",
    "\"\"\"\n",
    "# ===================================== Step 4: Classification & Clustering ==========================================\n",
    "# ----------------------------------------- Step 4: Classification ---------------------------------------------------\n",
    "\n",
    "Accuracy_Train, Cr_Train, Accuracy_Test, Cr_Test = output_network(Data, Labels, model, type_class, k_fold=5)\n",
    "\"\"\"\n",
    "type_class: 'KNN', 'LR', 'MLP', 'SVM', 'DT', 'NB', 'RF', 'AdaBoost', 'XGBoost', 'LDA'\n",
    "\n",
    "LR: LogisticRegression; MLP: Multilayer perceptron, SVM:Support Vector Machine; DT: Decision Tree; NB: Naive Bayes;\n",
    "RF: Random Forest; AdaBoost; XGBoost; LDA: Linear Discriminant Analysis; KNN:K-Nearest Neighbors \n",
    "Parameters:\n",
    "The number of hidden layers: hidden_layer_mlp; The number of epochs MLP: max_iter,\n",
    "\n",
    "max_depth=The maximum depth of the tree, random forest and XGBoost;;\n",
    "n_estimators:The number of trees in the forest.\n",
    "\"\"\"\n",
    "# ----------------------------------------- Step 4: Clustering -----------------------------------------------------\n",
    "clustering(Data, n_clusters=3, max_iter=100, thr_brich=0.5,  branchfactor_brich=50, n_neighbor_SpecCluster=10,\n",
    "           minsamples_optics=15, max_dist_optics=5, batch_size_MBKmeans=10, type_cluster='MiniBatchKMeans')\n",
    "\"\"\"\n",
    "type_cluster: kmeans; Agglomerative; DBSCAN; GMM:Gaussian Mixture Models; Meanshift; Birch; SpectralClustering; \n",
    "OPTICS; MiniBatchKMeans\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def classification(data, labels, type_class, hidden_layer_mlp, max_iter, kernel_svm, c_svm, gamma_svm, max_depth, criterion_dt, n_estimators):\n",
    "   \n",
    "\n",
    "   \n",
    "    return model, type_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.random.randn(1000)\n",
    "y = np.random.randn(1000)\n",
    "\n",
    "left, width = 0.1, 0.65\n",
    "bottom= 0.1\n",
    "width = 0.65\n",
    "height =0.65\n",
    "bottom_h =  left+width+0.02\n",
    "left_h   = left+width+0.02\n",
    "\n",
    "rect_scatter = [left, bottom, width, height]\n",
    "rect_histx   = [left, bottom_h, width, 0.2]\n",
    "rect_histy   = [left_h, bottom, 0.2, height] \n",
    "plt.figure(1, figsize=(8,8))\n",
    "axScatter = plt.axes(rect_scatter)\n",
    "axHistx   = plt.axes(rect_histx)\n",
    "axHisty   = plt.axes(rect_histy)\n",
    "\n",
    "plt.figure(1, figsize=(8,8))\n",
    "axScatter = plt.axes(rect_scatter)\n",
    "axHistx   = plt.axes(rect_histx)\n",
    "axHisty   = plt.axes(rect_histy)\n",
    "\n",
    "axScatter.scatter(x, y,c='r')\n",
    "\n",
    "binwidth = 0.25\n",
    "xymax = np.max( [np.max(np.fabs(x)), np.max(np.fabs(y))] )\n",
    "lim = ( int(xymax/binwidth) + 1) * binwidth\n",
    "axScatter.set_xlim( (-lim, lim) )\n",
    "axScatter.set_ylim( (-lim, lim) )\n",
    "bins = np.arange(-lim, lim + binwidth, binwidth)\n",
    "\n",
    "axHistx.hist(x, bins=bins,color='green')\n",
    "\n",
    "axHisty.hist(y, bins=bins, orientation='horizontal',color='b')\n",
    "\n",
    "axHistx.set_xlim( axScatter.get_xlim() );\n",
    "axHisty.set_ylim( axScatter.get_ylim() );"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets.samples_generator import make_blobs\n",
    "X, y = make_blobs(n_samples=50, centers=2, random_state=0, cluster_std=0.8)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
