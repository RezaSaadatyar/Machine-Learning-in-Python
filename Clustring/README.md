**Clustring**<br/>
Clustering is a type of unsupervised machine learning technique used to group similar data points together based on certain criteria or features. Clustering methods aim to discover hidden patterns or structures within the data. 

**Clustering Algorithms:**
 - `K-Means Clustering:`
   - It partitions the data into K clusters, where K is a user-defined parameter.
   - It minimizes the sum of squared distances between data points and the centroid of their assigned cluster.
   - K-Means is efficient and works well when clusters are spherical and have roughly equal sizes.
- `Hierarchical Clustering:`
   - Hierarchical clustering creates a tree-like structure of nested clusters, known as a dendrogram.
   - It does not require the user to specify the number of clusters beforehand.
   - Agglomerative hierarchical clustering starts with individual data points as clusters and merges them into larger clusters, while divisive hierarchical clustering starts with a single cluster and recursively divides it into smaller clusters.
- `DBSCAN (Density-Based Spatial Clustering of Applications with Noise):`
   - DBSCAN groups data points that are close together and separates data points in less dense regions.
   - It automatically determines the number of clusters based on density.
   - DBSCAN is robust to noise and can find clusters of arbitrary shapes.
- `Gaussian Mixture Model (GMM):`
   - GMM assumes that the data is generated from a mixture of multiple Gaussian distributions.
   - It estimates the parameters of these Gaussian distributions to model the data.
   - GMM can be used for density estimation as well as clustering.
- `Mean Shift Clustering:`
   - Mean Shift is a non-parametric clustering algorithm that finds modes or peaks in the data distribution.
   - It iteratively shifts data points towards the mode of their local density estimate.
   - It is effective in finding clusters with irregular shapes.
- `Spectral Clustering:`
   - Spectral clustering transforms the data into a lower-dimensional space using techniques from linear algebra (e.g., eigenvalue decomposition) and then applies K-Means or another clustering algorithm.
   - It can discover non-convex clusters and is suitable for data with complex structures.
- `Density Peak Clustering (DPC):`
   - DPC identifies cluster centers and assigns data points to clusters based on the density of data points around these centers.
   - It is particularly useful for datasets with varying cluster densities.
- `Affinity Propagation:`
   - Affinity Propagation is a message-passing algorithm that identifies data points that are exemplars of clusters.
   - It doesn't require the specification of the number of clusters and can discover unevenly sized clusters.
- `Self-Organizing Maps (SOM):`
   - SOM is a neural network-based clustering technique that maps high-dimensional data to a lower-dimensional grid.
   - It preserves the topological relationships between data points and can be used for visualization.

**Evaluate different clustering techniques based on different criteria:**
- **Cluster Shape:**
   - `K-Means: `Assumes spherical clusters with roughly equal sizes.
   - `DBSCAN:` Can discover clusters of arbitrary shapes.
   - `Gaussian Mixture Model (GMM):` Assumes Gaussian-shaped clusters.
- **Number of Clusters**:
   - `K-Means:` Requires the user to specify the number of clusters (K) in advance.
   - `Hierarchical Clustering:` Does not require specifying K in advance; dendrogram can help determine the number of clusters.
   - `DBSCAN:` Automatically determines the number of clusters based on data density.
   - `GMM:` Can work with a specified number of components but also allows for estimating it.
- **Handling Noise**:
   - `K-Means:` Sensitive to noise; outliers can significantly affect cluster centroids.
   - `DBSCAN:` Robust to noise; can classify outliers as noise points.
   - `GMM:` Sensitive to outliers as it assumes Gaussian distributions.
- **Cluster Size**:
   - `K-Means:` Expects roughly equal-sized clusters.
   - `DBSCAN:` Can handle clusters of varying sizes.
   - `Affinity Propagation:` Can discover clusters of different sizes.
- **Cluster Density**:
   - `K-Means:` Struggles with unevenly sized or irregularly shaped clusters.
   - `DBSCAN:` Sensitive to density variations and can find clusters with varying densities.
   - `Spectral Clustering:` Can capture clusters with complex density patterns.
- **Scalability**:
   - `K-Means:` Scales well for large datasets.
   - `DBSCAN:` Scales well for moderate-sized datasets but can be slow for very large datasets.
   - `Mean Shift:` May not scale well for high-dimensional or large datasets.
- **Interpretability**:
   - `K-Means:` Cluster centers are easy to interpret.
   - `Hierarchical Clustering:` Dendrogram provides insights into hierarchy.
   - `GMM:` Provides probabilistic cluster assignments.
- **Initialization Sensitivity**:
   - `K-Means:` Sensitive to initial cluster center placement.
   - `GMM:` Sensitive to initial parameter estimates.
   - `DBSCAN:` Not sensitive to initialization.
- **Applicability**:
   - `K-Means:` Suitable for well-separated, isotropic clusters.
   - `DBSCAN:` Effective for discovering clusters in data with varying densities and arbitrary shapes.
   - `Spectral Clustering:` Useful for non-convex clusters and graph-based data.
   - `GMM:` Applicable when the data distribution can be modeled as a mixture of Gaussians.
- **Computation Complexity**:
    - `K-Means:` Typically has linear complexity with the number of data points and clusters.
    - `DBSCAN:` Has a complexity of O(n^2) in the worst case for naive implementations.
    - `Spectral Clustering:` Involves eigenvalue decomposition, which can be computationally expensive.